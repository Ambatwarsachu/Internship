{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d2648f",
   "metadata": {},
   "source": [
    "## Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and \n",
    "salary filter. \n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs \n",
    "The task will be done as shown in the below steps: \n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field. \n",
    "3. Then click the search button. \n",
    "4. Then apply the location filter and salary filter by checking the respective boxes \n",
    "5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b088351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome() ## Initialize the driver from chrome browser\n",
    "\n",
    "driver.get('https://www.naukri.com/') #opening naukri site\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME, \"suggestor-input \") \n",
    "designation.send_keys('Data Scientist') # Enter \"Data Scientist\" in the search field\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search.click() # Click the search button\n",
    "\n",
    "location_filter = driver.find_element(By.XPATH, '/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[4]/div[2]/div[3]/label')\n",
    "location_filter.click() # Apply the \"Delhi/NCR\" location filter\n",
    "\n",
    "salary_filter = driver.find_element(By.XPATH, '/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "salary_filter.click() # Apply the \"3-6 Lakhs\" salary filter\n",
    "\n",
    "#creating empty list for store the value\n",
    "job_title = []\n",
    "job_location= []\n",
    "company_name = []\n",
    "experience_required= []\n",
    "\n",
    "#scrape the first 10 records of job title\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title \"]')\n",
    "for i in title_tags:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "job_title[0:10]\n",
    "\n",
    "#scrape the first 10 records of Location\n",
    "location_tags = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "for i in location_tags:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "job_location[0:10]\n",
    "\n",
    "##scrape the first 10 records of Company Name\n",
    "company_tags = driver.find_elements(By.XPATH, '//a[@class=\" comp-name mw-25\"]')\n",
    "for i in company_tags:\n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "company_name[0:10]\n",
    "\n",
    "##scrape the first 10 records of experience required\n",
    "exp_req = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "for i in exp_req:\n",
    "    exp = i.text\n",
    "    experience_required.append(exp)\n",
    "experience_required[0:10]\n",
    "\n",
    "import pandas as pd\n",
    "#creating Dataframe for above data\n",
    "df = pd.DataFrame({'Title':job_title,'Location':job_location, 'Company Name':company_name, 'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21d802",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the \n",
    "#job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "#3. Then click the searchbutton. \n",
    "#4. Then scrape the data for the first 10 jobs results you get. \n",
    "#5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5917dd51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[3]/div/div/button\"}\n  (Session info: chrome=129.0.6668.58); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF779F9FDA5+29557]\n\t(No symbol) [0x00007FF779F12240]\n\t(No symbol) [0x00007FF779DCB6EA]\n\t(No symbol) [0x00007FF779E1FA15]\n\t(No symbol) [0x00007FF779E1FC6C]\n\t(No symbol) [0x00007FF779E6BB07]\n\t(No symbol) [0x00007FF779E4753F]\n\t(No symbol) [0x00007FF779E688A3]\n\t(No symbol) [0x00007FF779E472A3]\n\t(No symbol) [0x00007FF779E112DF]\n\t(No symbol) [0x00007FF779E12451]\n\tGetHandleVerifier [0x00007FF77A2CDCBD+3363469]\n\tGetHandleVerifier [0x00007FF77A319B47+3674391]\n\tGetHandleVerifier [0x00007FF77A30EAEB+3629243]\n\tGetHandleVerifier [0x00007FF77A05FC66+815670]\n\t(No symbol) [0x00007FF779F1D6EF]\n\t(No symbol) [0x00007FF779F192B4]\n\t(No symbol) [0x00007FF779F19450]\n\t(No symbol) [0x00007FF779F081FF]\n\tBaseThreadInitThunk [0x00007FFD7FEB257D+29]\n\tRtlUserThreadStart [0x00007FFD8148AF28+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[436], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome() \u001b[38;5;66;03m# Set up the driver for chrome\u001b[39;00m\n\u001b[0;32m      3\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.shine.com/\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#open the Shine.com webpage\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m allow \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div/div[3]/div/div/button\u001b[39m\u001b[38;5;124m'\u001b[39m ) \u001b[38;5;66;03m#we are can celling one pop up that is comming after site open\u001b[39;00m\n\u001b[0;32m      6\u001b[0m allow\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m      8\u001b[0m deg \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#Enter \"Data Syientist\" in the \"Job title, Skills\" field \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:748\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    745\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    746\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[3]/div/div/button\"}\n  (Session info: chrome=129.0.6668.58); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF779F9FDA5+29557]\n\t(No symbol) [0x00007FF779F12240]\n\t(No symbol) [0x00007FF779DCB6EA]\n\t(No symbol) [0x00007FF779E1FA15]\n\t(No symbol) [0x00007FF779E1FC6C]\n\t(No symbol) [0x00007FF779E6BB07]\n\t(No symbol) [0x00007FF779E4753F]\n\t(No symbol) [0x00007FF779E688A3]\n\t(No symbol) [0x00007FF779E472A3]\n\t(No symbol) [0x00007FF779E112DF]\n\t(No symbol) [0x00007FF779E12451]\n\tGetHandleVerifier [0x00007FF77A2CDCBD+3363469]\n\tGetHandleVerifier [0x00007FF77A319B47+3674391]\n\tGetHandleVerifier [0x00007FF77A30EAEB+3629243]\n\tGetHandleVerifier [0x00007FF77A05FC66+815670]\n\t(No symbol) [0x00007FF779F1D6EF]\n\t(No symbol) [0x00007FF779F192B4]\n\t(No symbol) [0x00007FF779F19450]\n\t(No symbol) [0x00007FF779F081FF]\n\tBaseThreadInitThunk [0x00007FFD7FEB257D+29]\n\tRtlUserThreadStart [0x00007FFD8148AF28+40]\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome() # Set up the driver for chrome\n",
    "\n",
    "driver.get('https://www.shine.com/') #open the Shine.com webpage\n",
    "\n",
    "allow = driver.find_element(By.XPATH,'/html/body/div/div[3]/div/div/button' ) #we are can celling one pop up that is comming after site open\n",
    "allow.click()\n",
    "\n",
    "deg = driver.find_element(By.CLASS_NAME, \"input\") #Enter \"Data Syientist\" in the \"Job title, Skills\" field \n",
    "deg.send_keys('Data Scientist')\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME, \"iconH-zoom-white\")\n",
    "search.click() #clicking on the serach button\n",
    "\n",
    "job = driver.find_element(By.CLASS_NAME, \"form-control  \")\n",
    "job.send_keys('Data Scientist') #new popup is comming for entering field data as Data Scientist\n",
    "\n",
    "loc = driver.find_element(By.ID,\"id_loc\")\n",
    "loc.send_keys('Bangalore') #entering Bangalore data in location field, searching By ID(insted of CLASS NAME)\n",
    "\n",
    "searchs = driver.find_element(By.XPATH, '/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button')\n",
    "searchs.click() #clicking on the serach button\n",
    "\n",
    "#creating empty List for below\n",
    "\n",
    "job_title = []\n",
    "job_locationn= []\n",
    "company_namee = []\n",
    "experience_required= []\n",
    "\n",
    "close_Button = driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div[1]/div/div/button')\n",
    "close_Button.click() # closing one popup page\n",
    "\n",
    "# Find all elements matching the Job_Title requirement XPath\n",
    "title_job = driver.find_elements(By.XPATH, '//strong[@class=\"jobCard_pReplaceH2__xWmHg\"]')\n",
    "\n",
    "# Loop through each element and extract the text (Job_Title details)\n",
    "for i in title_job:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# Print the number of Job_Title records scraped\n",
    "print(len(job_title))\n",
    "\n",
    "# Find all elements matching the Location requirement XPath\n",
    "loc_job = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "\n",
    "# Loop through each element and extract the text (Location details)\n",
    "for i in loc_job:\n",
    "    loc = i.text\n",
    "    job_locationn.append(loc)\n",
    "    \n",
    "# Print the number of Location records scraped\n",
    "print(len(job_locationn))\n",
    "\n",
    "# Find all elements matching the Company Name requirement XPath\n",
    "name_comp = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "\n",
    "# Loop through each element and extract the text (Company Name details)\n",
    "for i in name_comp:\n",
    "    comp = i.text\n",
    "    company_namee.append(comp)\n",
    "    \n",
    "# Print the number of Company Name records scraped\n",
    "    print(len(company_namee))\n",
    "\n",
    "# Find all elements matching the experience requirement XPath\n",
    "exp_requ = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "\n",
    "# Loop through each element and extract the text (experience details)\n",
    "for i in exp_requ:\n",
    "    exp = i.text\n",
    "    experience_required.append(exp)\n",
    "\n",
    "# Print the number of experience records scraped\n",
    "print(len(experience_required))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_title,\n",
    "    'Location': job_locationn,\n",
    "    'Company Name': company_namee,\n",
    "    'Experience': experience_required\n",
    "})\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "df.to_csv('job_data.csv', index=False)\n",
    "\n",
    "# Print the first 10 records\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7f31a",
   "metadata": {},
   "source": [
    "## Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b822cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8294dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "29053b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1560f8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages found or unable to click 'Next'.\n",
      "   Rating       Review Summary  \\\n",
      "0       5  Best in the market!   \n",
      "1       5             Terrific   \n",
      "2       5       Classy product   \n",
      "3       5    Worth every penny   \n",
      "4       5     Perfect product!   \n",
      "5       5    Terrific purchase   \n",
      "6       5            Wonderful   \n",
      "7       5            Excellent   \n",
      "8       5            Brilliant   \n",
      "9       5     Perfect product!   \n",
      "10      5              Awesome   \n",
      "11      5            Must buy!   \n",
      "12      5            Fabulous!   \n",
      "13      5            Just wow!   \n",
      "14      5            Fabulous!   \n",
      "15      5    Terrific purchase   \n",
      "16      5            Brilliant   \n",
      "17      5       Simply awesome   \n",
      "18      5               Super!   \n",
      "19      5   Highly recommended   \n",
      "20      5            Wonderful   \n",
      "21      5            Brilliant   \n",
      "22      5    Terrific purchase   \n",
      "23      5            Brilliant   \n",
      "24      5       Classy product   \n",
      "25      5            Excellent   \n",
      "26      5     Perfect product!   \n",
      "27      5            Brilliant   \n",
      "28      5            Must buy!   \n",
      "29      5             Terrific   \n",
      "30      5            Just wow!   \n",
      "31      5    Worth every penny   \n",
      "\n",
      "                                          Full Review  \n",
      "0                                         Good Camera  \n",
      "1                                      Very very good  \n",
      "2   Camera is awesome\\nBest battery backup\\nA perf...  \n",
      "3   Feeling awesome after getting the delivery of ...  \n",
      "4                                        Photos super  \n",
      "5                                   Value for money üòç  \n",
      "6                              This is amazing at all  \n",
      "7                                                 NYC  \n",
      "8                            very good camera quality  \n",
      "9                                          V Good all  \n",
      "10  iPhone 11 is a good phone. Not a very big diff...  \n",
      "11                                It‚Äôs really awesome  \n",
      "12  It‚Äôs very good battery life and display and vi...  \n",
      "13                                  Perfect Product!!  \n",
      "14                    Superüî• and good performance üëå‚ù§Ô∏è  \n",
      "15                                 Value for money üñ§üñ§  \n",
      "16                                   Excellent Phone.  \n",
      "17  Really satisfied with the Product I received.....  \n",
      "18                        Good product üëåI love iPhone  \n",
      "19  My first ever I phone and awesome performance ...  \n",
      "20  Excellent Fabulous Adorable Iphone 11 Value fo...  \n",
      "21  Perfect iPhone on this budget!! Camera and the...  \n",
      "22                            Awesome Thanks Flipkart  \n",
      "23                                         Best phone  \n",
      "24                 Outstanding performance this phone  \n",
      "25  A perfect phone and a good battery super camer...  \n",
      "26                         Very nice iPhone 11 i lake  \n",
      "27                                    Fantastic phone  \n",
      "28                                          Excellent  \n",
      "29  Really worth of money. i just love it. It is t...  \n",
      "30  Best in class. Battery backup is good especial...  \n",
      "31  I love the design better than iPhone 12 in loo...  \n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'iphone11_flipkart_reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[282], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Optionally, save the data to a CSV file\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miphone11_flipkart_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'iphone11_flipkart_reviews.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_reviews():\n",
    "    # Scrape Ratings\n",
    "    rating_elements = driver.find_elements(By.XPATH, '//div[@class=\"XQDdHH Ga3i8K\"]')\n",
    "    for rating in rating_elements:\n",
    "        ratings.append(rating.text)\n",
    "\n",
    "    # Scrape Review Summaries\n",
    "    summary_elements = driver.find_elements(By.XPATH, '//p[@class=\"z9E0IG\"]')\n",
    "    for summary in summary_elements:\n",
    "        review_summaries.append(summary.text)\n",
    "\n",
    "    # Scrape Full Reviews\n",
    "    full_review_elements = driver.find_elements(By.XPATH, '//div[@class=\"ZmyHeo\"]')\n",
    "    for review in full_review_elements:\n",
    "        full_reviews.append(review.text)\n",
    "\n",
    "# Loop to scrape multiple pages until 100 reviews are gathered\n",
    "while len(ratings) < 100:\n",
    "    scrape_reviews()\n",
    "\n",
    "    # Click on the 'Next' button to go to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]')\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "    except:\n",
    "        print(\"No more pages found or unable to click 'Next'.\")\n",
    "        break\n",
    "\n",
    "# Ensure we only have the first 100 reviews\n",
    "#ratings = ratings[:100]\n",
    "#review_summaries = review_summaries[:100]\n",
    "#full_reviews = full_reviews[:100]\n",
    "\n",
    "# Create a DataFrame to store the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Rating': ratings,\n",
    "    'Review Summary': review_summaries,\n",
    "    'Full Review': full_reviews\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the data to a CSV file\n",
    "df.to_csv('iphone11_flipkart_reviews.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bcf1c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7b28d",
   "metadata": {},
   "source": [
    "## Q4: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for ‚Äúsneakers‚Äù inthe search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7817ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Brand_Name                                 ProductDescription   Price\n",
      "0    Red Tape  BLACK SHOES PREMIUM QUALITY MEN Sneakers Casua...  ‚Çπ1,079\n",
      "1      lejano                            Casual Sneakers For Men    ‚Çπ449\n",
      "2        ATOM                        KLR-009 01 Sneakers For Men    ‚Çπ999\n",
      "3       Abros                                   Sneakers For Men    ‚Çπ599\n",
      "4      corsac  Combo Pack Of 2 Men‚Äôs Casual Shoes Sneakers Fo...    ‚Çπ639\n",
      "..        ...                                                ...     ...\n",
      "95   Red Tape                                                     ‚Çπ2,459\n",
      "96    MACTREE                                                       ‚Çπ676\n",
      "97      Zixer                                                       ‚Çπ599\n",
      "98      BIRDE                                                       ‚Çπ784\n",
      "99       ATOM                                                     ‚Çπ1,099\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Flipkart and search for sneakers\n",
    "driver.get('https://www.flipkart.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Close login popup if present (handle cases where it pops up)\n",
    "try:\n",
    "    close_popup = driver.find_element(By.XPATH, '//button[contains(text(),\"‚úï\")]')\n",
    "    close_popup.click()\n",
    "except:\n",
    "    pass  # If popup isn't there, just proceed\n",
    "\n",
    "# Enter the search term\n",
    "search = driver.find_element(By.CLASS_NAME, \"Pke_EE\")\n",
    "search.send_keys('sneakers')\n",
    "\n",
    "# Click the search button\n",
    "search_button = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button')\n",
    "search_button.click()\n",
    "\n",
    "# Initialize lists\n",
    "Brand_Name = []\n",
    "ProductDescription = []\n",
    "Price = []\n",
    "\n",
    "# Define the scraping function\n",
    "def scrape_shoes():\n",
    "    # Scraping brand names\n",
    "    Brand = driver.find_elements(By.XPATH, '//div[@class=\"syl9yP\"]')\n",
    "    for i in Brand:\n",
    "        Brand_Name.append(i.text if i.text else '')  # Append empty string if no text\n",
    "    \n",
    "    # Scraping product descriptions\n",
    "    pro_des = driver.find_elements(By.XPATH, '//a[@class=\"WKTcLC BwBZTg\"]')\n",
    "    for j in pro_des:\n",
    "        ProductDescription.append(j.text if j.text else '')  # Append empty string if no text\n",
    "    \n",
    "    # Scraping prices\n",
    "    prc = driver.find_elements(By.XPATH, '//div[@class=\"Nx9bqj\"]')\n",
    "    for k in prc:\n",
    "        Price.append(k.text if k.text else '')  # Append empty string if no text\n",
    "\n",
    "# Pagination loop\n",
    "page_num = 1\n",
    "while len(Brand_Name) < 100:\n",
    "    scrape_shoes()\n",
    "    \n",
    "    # Stop if we already have 100 items\n",
    "    if len(Brand_Name) >= 100:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        page_num += 1\n",
    "        # Get the \"Next\" button and navigate to the next page\n",
    "        next_page = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "        driver.get(next_page.get_attribute('href'))  # Extract href and navigate\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        print('No more pages')\n",
    "        break\n",
    "\n",
    "# Ensure all lists are the same length (fill with empty strings if necessary)\n",
    "max_len = max(len(Brand_Name), len(ProductDescription), len(Price))\n",
    "#Brand_Name += [''] * (max_len - len(Brand_Name))\n",
    "ProductDescription += [''] * (max_len - len(ProductDescription))\n",
    "#Price += [''] * (max_len - len(Price))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Brand_Name': Brand_Name[:100], 'ProductDescription': ProductDescription[:100], 'Price': Price[:100]})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the data to a excel file\n",
    "#df.to_excel(\"sneakers_data.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667ebe8",
   "metadata": {},
   "source": [
    "## Q5: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU\n",
    "Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:\n",
    "    Aftersetting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a79958d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.maximize_window()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "328a7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "amzn.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ea16d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH, '/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "2c393a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_i7 = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/span/div/div/div[5]/ul[2]/span/span[11]/li/span/a/span')\n",
    "core_i7.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4e7b1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = []\n",
    "Ratings = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "1a24175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title              Ratings  \\\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...  4.2 out of 5 stars.   \n",
      "1  Dell [Smartchoice] Inspiron 5430 Thin & Light ...  3.8 out of 5 stars.   \n",
      "2  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...   3.6 out of 5 stars   \n",
      "3  WALKENT Stylish 15.6\" Laptop Bag, Premium Leat...   3.3 out of 5 stars   \n",
      "4  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...   3.6 out of 5 stars   \n",
      "5  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...   4.2 out of 5 stars   \n",
      "6  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...   4.1 out of 5 stars   \n",
      "7  (Refurbished) Dell Latitude 7480 14in FHD Lapt...   5.0 out of 5 stars   \n",
      "8  ASUS TUF Gaming F15, 15.6\" (39.62cm) FHD 144Hz...   3.9 out of 5 stars   \n",
      "9  Dell Inspiron 3530 Laptop, 13th Generation Int...   4.0 out of 5 stars   \n",
      "\n",
      "      Price  \n",
      "0    59,990  \n",
      "1    75,490  \n",
      "2    49,990  \n",
      "3     2,990  \n",
      "4    76,990  \n",
      "5    71,990  \n",
      "6    55,990  \n",
      "7    27,531  \n",
      "8  1,07,999  \n",
      "9    69,890  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize lists to store scraped data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Scrape the first 10 laptops' data\n",
    "for i in range(10):\n",
    "    # Scrape title\n",
    "    try:\n",
    "        title = driver.find_elements(By.XPATH, '//span[@class=\"a-size-medium a-color-base a-text-normal\"]')[i].text\n",
    "        titles.append(title)\n",
    "    except:\n",
    "        titles.append(\"\")\n",
    "\n",
    "    # Scrape rating\n",
    "    try:\n",
    "        rating = driver.find_elements(By.XPATH, '//span[@class=\"a-icon-alt\"]')[i].get_attribute('innerHTML')\n",
    "        ratings.append(rating)\n",
    "    except:\n",
    "        ratings.append(\"\")\n",
    "\n",
    "    # Scrape price\n",
    "    try:\n",
    "        price = driver.find_elements(By.XPATH, '//span[@class=\"a-price-whole\"]')[i].text\n",
    "        prices.append(price)\n",
    "    except:\n",
    "        prices.append(\"\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame to organize the data\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the data to an Excel file\n",
    "df.to_excel(\"laptop_data_amazon.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0eb11",
   "metadata": {},
   "source": [
    "## Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "097fe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "top_quot = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top_quot.click()\n",
    "\n",
    "quites = []\n",
    "Author = []\n",
    "Type_Of_Quote = []\n",
    "\n",
    "Quote = driver.find_elements(By.XPATH, '//a[@class=\"title\"]')\n",
    "for i in Quote:\n",
    "    qut = i.text\n",
    "    quites.append(qut)\n",
    "print(len(quites))\n",
    "\n",
    "authr = driver.find_elements(By.XPATH, '//div[@class=\"author\"]')\n",
    "for j in authr:\n",
    "    aut = j.text\n",
    "    Author.append(aut)\n",
    "print(len(Author))\n",
    "\n",
    "toq = driver.find_elements(By.XPATH, '//div[@class=\"tags\"]')\n",
    "for k in toq:\n",
    "    to = k.text\n",
    "    Type_Of_Quote.append(to)\n",
    "print(len(Type_Of_Quote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc54c12",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'Quote':quites, 'Author':Author, 'Type Of Quote':Type_Of_Quote})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "f5794aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Total Quotes Scraped: 1000\n",
      "Total Authors Scraped: 1000\n",
      "Total Types of Quote Scraped: 1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the website\n",
    "driver.get('https://www.azquotes.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Click on the \"Top Quotes\" link\n",
    "top_quot = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top_quot.click()\n",
    "\n",
    "# Initialize lists to store data\n",
    "quites = []\n",
    "Author = []\n",
    "Type_Of_Quote = []\n",
    "\n",
    "# Function to scrape data from the current page\n",
    "def scrape_page():\n",
    "    # Scrape Quotes\n",
    "    Quote = driver.find_elements(By.XPATH, '//a[@class=\"title\"]')\n",
    "    for i in Quote:\n",
    "        quites.append(i.text)\n",
    "\n",
    "    # Scrape Authors\n",
    "    authr = driver.find_elements(By.XPATH, '//div[@class=\"author\"]')\n",
    "    for j in authr:\n",
    "        Author.append(j.text)\n",
    "\n",
    "    # Scrape Type of Quote (Tags)\n",
    "    toq = driver.find_elements(By.XPATH, '//div[@class=\"tags\"]')\n",
    "    for k in toq:\n",
    "        Type_Of_Quote.append(k.text)\n",
    "\n",
    "# Scrape data until we have at least 1000 quotes\n",
    "page_num = 1\n",
    "while len(quites) < 1000:\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    scrape_page()\n",
    "    \n",
    "    if len(quites) >= 1000:\n",
    "        break\n",
    "    \n",
    "    # Go to the next page\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[4]/li[12]/a')\n",
    "        next_page.click()\n",
    "        page_num += 1\n",
    "        time.sleep(2)  # wait for the page to load\n",
    "    except:\n",
    "        print(\"No more pages available\")\n",
    "        break\n",
    "\n",
    "# Limit the lists to 1000 items\n",
    "quites = quites[:1000]\n",
    "Author = Author[:1000]\n",
    "Type_Of_Quote = Type_Of_Quote[:1000]\n",
    "\n",
    "# Print the number of items scraped\n",
    "print(f\"Total Quotes Scraped: {len(quites)}\")\n",
    "print(f\"Total Authors Scraped: {len(Author)}\")\n",
    "print(f\"Total Types of Quote Scraped: {len(Type_Of_Quote)}\")\n",
    "\n",
    "# Optionally save the data in a DataFrame and save it as a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Quote': quites, 'Author': Author, 'Type Of Quote': Type_Of_Quote})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "db435255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type Of Quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "      <td>Sydney J. Harris</td>\n",
       "      <td>Love, Inspirational, Motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>America... just a nation of two hundred millio...</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Gun, Two, Qualms About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>For every disciplined effort there is a multip...</td>\n",
       "      <td>Jim Rohn</td>\n",
       "      <td>Inspirational, Greatness, Best Effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The spiritual journey is individual, highly pe...</td>\n",
       "      <td>Ram Dass</td>\n",
       "      <td>Spiritual, Truth, Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The mind is not a vessel to be filled but a fi...</td>\n",
       "      <td>Plutarch</td>\n",
       "      <td>Inspirational, Leadership, Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Quote              Author  \\\n",
       "0    The essence of strategy is choosing what not t...      Michael Porter   \n",
       "1    One cannot and must not try to erase the past ...          Golda Meir   \n",
       "2    Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
       "3    Death is something inevitable. When a man has ...      Nelson Mandela   \n",
       "4    You have to love a nation that celebrates its ...        Erma Bombeck   \n",
       "..                                                 ...                 ...   \n",
       "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
       "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
       "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
       "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
       "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
       "\n",
       "                                Type Of Quote  \n",
       "0    Essence, Deep Thought, Transcendentalism  \n",
       "1                   Inspiration, Past, Trying  \n",
       "2                         Country, Peace, War  \n",
       "3          Inspirational, Motivational, Death  \n",
       "4                4th Of July, Food, Patriotic  \n",
       "..                                        ...  \n",
       "995         Love, Inspirational, Motivational  \n",
       "996                    Gun, Two, Qualms About  \n",
       "997     Inspirational, Greatness, Best Effort  \n",
       "998                    Spiritual, Truth, Yoga  \n",
       "999      Inspirational, Leadership, Education  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b5047",
   "metadata": {},
   "source": [
    "## Q7: Write a python program to display list of respected former Prime Ministers of India \n",
    "(i.e. Name,Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1\n",
    "scrap the mentioned data and make the DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0e692098",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6e4cba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.jagranjosh.com/general-knowledge/list-of%02all-prime-ministers-of-india-1473165149-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "08ef633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f627b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name                Born-Dead      Term of Office           Remarks\n",
      "0     1         Jawaharlal Nehru  57 years, 274 days     15 August1947\n",
      "1     2         Gulzarilal Nanda  65 years, 328 days       27 May 1964\n",
      "2     3      Lal Bahadur Shastri  62 years, 250 days       9 June 1964\n",
      "3     4         Gulzarilal Nanda  67 years, 191 days   11 January 1966\n",
      "4     4            Indira Gandhi   48 years, 66 days   24 January 1966\n",
      "5     5            Morarji Desai   81 years, 24 days     24 March 1977\n",
      "6     6             Charan Singh  76 years, 217 days      28 July 1979\n",
      "7     7            Indira Gandhi   62 years, 56 days   14 January 1980\n",
      "8     8             Rajiv Gandhi   40 years, 72 days   31 October 1984\n",
      "9     9  Vishwanath Pratap Singh  58 years, 160 days   2 December 1989\n",
      "10   10          Chandra Shekhar  63 years, 207 days  10 November 1990\n",
      "11   11      P. V. Narasimha Rao  69 years, 358 days      21 June 1991\n",
      "12   12     Atal Bihari Vajpayee  71 years, 143 days       16 May 1996\n",
      "13   13         H. D. Deve Gowda   63 years, 14 days       1 June 1996\n",
      "14   14       Inder Kumar Gujral  77 years, 138 days     21 April 1997\n",
      "15   15     Atal Bihari Vajpayee   73 years, 84 days     19 March 1998\n",
      "16   16           Manmohan Singh  71 years, 239 days       22 May 2004\n",
      "17   17            Narendra Modi  63 years, 251 days       26 May 2014\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'former_prime_ministers_of_india.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[387], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to an Excel file\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformer_prime_ministers_of_india.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2335\u001b[0m     df,\n\u001b[0;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2344\u001b[0m )\n\u001b[1;32m-> 2345\u001b[0m formatter\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m   2346\u001b[0m     excel_writer,\n\u001b[0;32m   2347\u001b[0m     sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m   2348\u001b[0m     startrow\u001b[38;5;241m=\u001b[39mstartrow,\n\u001b[0;32m   2349\u001b[0m     startcol\u001b[38;5;241m=\u001b[39mstartcol,\n\u001b[0;32m   2350\u001b[0m     freeze_panes\u001b[38;5;241m=\u001b[39mfreeze_panes,\n\u001b[0;32m   2351\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m   2352\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2353\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m   2354\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:946\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    942\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(  \u001b[38;5;66;03m# type: ignore[abstract]\u001b[39;00m\n\u001b[0;32m    947\u001b[0m         writer,\n\u001b[0;32m    948\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    949\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    950\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    951\u001b[0m     )\n\u001b[0;32m    952\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m     path,\n\u001b[0;32m     63\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m     64\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m     65\u001b[0m     if_sheet_exists\u001b[38;5;241m=\u001b[39mif_sheet_exists,\n\u001b[0;32m     66\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1263\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[0;32m   1260\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m   1261\u001b[0m )\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1264\u001b[0m         path, mode, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m     )\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'former_prime_ministers_of_india.xlsx'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up WebDriver (ensure you have the correct path to your chromedriver or geckodriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1')\n",
    "\n",
    "# Maximize the window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Wait for the page to load fully\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the table containing Prime Ministers' information\n",
    "table = driver.find_element(By.XPATH, '//table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Find all rows of the table (excluding the header)\n",
    "rows = table.find_elements(By.XPATH, './/tr')[1:]  # Skip header row\n",
    "\n",
    "# Loop through each row and extract data\n",
    "for row in rows:\n",
    "    cols = row.find_elements(By.XPATH, './/td')\n",
    "    \n",
    "    # Extract the columns (Name, Born-Dead, Term of Office, Remarks)\n",
    "    names.append(cols[0].text.strip())\n",
    "    born_dead.append(cols[1].text.strip())\n",
    "    terms_of_office.append(cols[2].text.strip())\n",
    "    remarks.append(cols[3].text.strip())\n",
    "\n",
    "# Create a DataFrame using the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms_of_office,\n",
    "    'Remarks': remarks\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('former_prime_ministers_of_india.xlsx', index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "0ffb1d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row due to unexpected format: S.N Name Age when assumed office Term of office Duration\n",
      "Skipping row due to unexpected format: 1 Jawaharlal Nehru 57 years, 274 days 15 August1947 27 May 1964 16 years, 286 days\n",
      "Skipping row due to unexpected format: 2 Gulzarilal Nanda 65 years, 328 days 27 May 1964 9 June 1964 13 days\n",
      "Skipping row due to unexpected format: 3 Lal Bahadur Shastri 62 years, 250 days 9 June 1964 11 January 1966 1 year, 216 days\n",
      "Skipping row due to unexpected format: 4 Gulzarilal Nanda 67 years, 191 days 11 January 1966 24 January 1966 13 days\n",
      "Skipping row due to unexpected format: 4 Indira Gandhi 48 years, 66 days 24 January 1966 24 March 1977 11 years, 59 days\n",
      "Skipping row due to unexpected format: 5 Morarji Desai 81 years, 24 days 24 March 1977 28 July 1979 2 years, 126 days\n",
      "Skipping row due to unexpected format: 6 Charan Singh 76 years, 217 days 28 July 1979 14 January 1980 170 days\n",
      "Skipping row due to unexpected format: 7 Indira Gandhi 62 years, 56 days 14 January 1980 31 October 1984 4 years, 291 days\n",
      "Skipping row due to unexpected format: 8 Rajiv Gandhi 40 years, 72 days 31 October 1984 2 December 1989 5 years, 32 days\n",
      "Skipping row due to unexpected format: 9 Vishwanath Pratap Singh 58 years, 160 days 2 December 1989 10 November 1990 343 days\n",
      "Skipping row due to unexpected format: 10 Chandra Shekhar 63 years, 207 days 10 November 1990 21 June 1991 223 days\n",
      "Skipping row due to unexpected format: 11 P. V. Narasimha Rao 69 years, 358 days 21 June 1991 16 May 1996 4 years, 330 days\n",
      "Skipping row due to unexpected format: 12 Atal Bihari Vajpayee 71 years, 143 days 16 May 1996 1 June 1996 16 days\n",
      "Skipping row due to unexpected format: 13 H. D. Deve Gowda 63 years, 14 days 1 June 1996 21 April 1997 324 days\n",
      "Skipping row due to unexpected format: 14 Inder Kumar Gujral 77 years, 138 days 21 April 1997 19 March 1998 332 days\n",
      "Skipping row due to unexpected format: 15 Atal Bihari Vajpayee 73 years, 84 days 19 March 1998 22 May 2004 6 years, 64 days\n",
      "Skipping row due to unexpected format: 16 Manmohan Singh 71 years, 239 days 22 May 2004 26 May 2014 10 years, 4 days\n",
      "Skipping row due to unexpected format: 17 Narendra Modi 63 years, 251 days 26 May 2014 Incumbent 10 years, 87 days\n",
      "   Name Born-Dead Term of Office Remarks\n",
      "0   N/A       N/A            N/A     N/A\n",
      "1   N/A       N/A            N/A     N/A\n",
      "2   N/A       N/A            N/A     N/A\n",
      "3   N/A       N/A            N/A     N/A\n",
      "4   N/A       N/A            N/A     N/A\n",
      "5   N/A       N/A            N/A     N/A\n",
      "6   N/A       N/A            N/A     N/A\n",
      "7   N/A       N/A            N/A     N/A\n",
      "8   N/A       N/A            N/A     N/A\n",
      "9   N/A       N/A            N/A     N/A\n",
      "10  N/A       N/A            N/A     N/A\n",
      "11  N/A       N/A            N/A     N/A\n",
      "12  N/A       N/A            N/A     N/A\n",
      "13  N/A       N/A            N/A     N/A\n",
      "14  N/A       N/A            N/A     N/A\n",
      "15  N/A       N/A            N/A     N/A\n",
      "16  N/A       N/A            N/A     N/A\n",
      "17  N/A       N/A            N/A     N/A\n",
      "18  N/A       N/A            N/A     N/A\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1')\n",
    "\n",
    "# Maximize the window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Wait for the page to load fully\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the table containing Prime Ministers' information\n",
    "table = driver.find_element(By.XPATH, '//table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Find all rows of the table (excluding the header)\n",
    "rows = table.find_elements(By.XPATH, './/tbody/tr')  # Ensuring we get only the table body rows\n",
    "\n",
    "# Loop through each row and extract data\n",
    "for row in rows:\n",
    "    cols = row.find_elements(By.XPATH, './/td')\n",
    "    \n",
    "    # Only process rows with exactly 4 columns\n",
    "    if len(cols) == 4:\n",
    "        names.append(cols[0].text.strip())\n",
    "        born_dead.append(cols[1].text.strip())\n",
    "        terms_of_office.append(cols[2].text.strip())\n",
    "        remarks.append(cols[3].text.strip())\n",
    "    else:\n",
    "        # In case there is a mismatch, append blank data\n",
    "        print(f\"Skipping row due to unexpected format: {row.text}\")\n",
    "        names.append('N/A')\n",
    "        born_dead.append('N/A')\n",
    "        terms_of_office.append('N/A')\n",
    "        remarks.append('N/A')\n",
    "\n",
    "# Create a DataFrame using the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms_of_office,\n",
    "    'Remarks': remarks\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "#df.to_excel('former_prime_ministers_of_india.xlsx', index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9147ab6",
   "metadata": {},
   "source": [
    "## Q8: Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ‚Äô50 most expensive cars‚Äô\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap thementioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "169c28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.motor1.com/')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1a6c436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH, '/html/body/div[9]/div[2]/div/div/div[3]/div/div/div/form/input')\n",
    "search.send_keys('50 most expensive cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5b1621f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element(By.XPATH, '/html/body/div[9]/div[2]/div/div/div[3]/div/div/div/form/button[1]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f232260a",
   "metadata": {},
   "source": [
    "sec_ser_btn = driver.find_element(By.XPATH, '/html/body/div[9]/div[6]/form/input[2]')\n",
    "sec_ser_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "9764be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = driver.find_element(By.XPATH, '/html/body/div[9]/div[9]/div/div[1]/div/div/div[1]/div/div[1]/h3/a')\n",
    "cars.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "1e05ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "car_name = []\n",
    "\n",
    "\n",
    "car = driver.find_elements(By.XPATH, '//h3[@class=\"subheader\"]')\n",
    "for i in car:\n",
    "    ca = i.text\n",
    "    car_name.append(ca)\n",
    "print(len(car_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "610cc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "price = []\n",
    "prc = driver.find_elements(By.XPATH, '/html/body/div[9]/div[7]/div[2]/div[1]/div[2]/div[2]/p[4]')\n",
    "for j in prc:\n",
    "    pr = j.text\n",
    "    price.append(pr)\n",
    "print(len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "63e7bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"But to find out which of these unattainable-to-most vehicles is the priciest, we've scoured the invoice sheets and uncovered 50 cars all worth $1 million or more. The group includes hybrid Ferrari convertibles, an American speed king, a few electric Hypercars, and a trio of one-of-a-kind Rolls-Royce cars built exclusively for royalty.\",\n",
       " 'You might not know the name Czinger yet, but this California-based boutique automaker is responsible for the $1.7 million 21C supercar. With 1,250 horsepower on tap thanks to a 2.9-liter twin-turbocharged V-8, the 21C can rocket to 60 miles per hour in 1.9 seconds and a top speed of around 281 miles per hour.',\n",
       " 'Much like the roof-less McLaren Elva, the Ferrari Monza SP1 and SP2 are all about al fresco motoring. As the name implies, the SP1 is the single-seater option while the SP2 has just enough room for you to bring along a friend. Both cars come powered by the same naturally aspirated 6.5-liter V-12 engine capable of 809 horsepower and 530 pound-feet. Unfortunately for US buyers, neither of these models are street legal in the States. But f you have the funds to afford the car‚Äôs $1.75 million price tag, getting onto a track probably won‚Äôt be an issue anyway.',\n",
       " \"To call the Hispano Suiza Carmen Boulogne beautiful would be... a stretch. But this interesting-looking supercar made its US debut at this year's Amelia Island Concours d'Elegance boasting a 1,114-horsepower electric powertrain and a 0-60 time of 2.6 seconds. The Boulogne version pictured here ‚Äì the lighter and more powerful of the two‚Äîcosts close to $1.9 million.\",\n",
       " \"The electric onslaught is coming. Bentley says it will be a fully electric automaker by 2030. But before the luxury brand transitions to battery power fully, the Mulliner Batur helps send the iconic W12 engine off in style. With 710 horsepower from that twin-turbocharged engine, the Batur is the most powerful Bentley ever. And since it's limited to just 18 examples globally, the Batur costs a whopping $2.0 million.\",\n",
       " 'As with a few other cars on this list, the Aston Martin Vulcan was a track-only special. The British supercar made its world debut in 2015, powered by a naturally aspirated 7.0-liter V-12 that produced 820 horsepower, and limited to just 24 examples worldwide. An even more track-focused AMR Pro model followed, but even the base Vulcan was a pricey commodity, costing about $2.4-million from the factory.',\n",
       " 'The Rimac Nevera takes the title of most expensive EV with its $2.4 million price tag‚Äîbut only by a few hundred thousand dollars over the next priciest electric supercar. Packing 1,914 horsepower and 1,740 pound-feet of torque, the Nevera can hit 60 in under 2.0 seconds and will continue on to a top speed of 258 miles per hour. Those lucky enough to order one should see it in their driveway before the end of 2022.',\n",
       " 'First came the Zonda, then the Huayra, and now the Pagani Utopia. With 852 horsepower and an available seven-speed manual gearbox, the Utopia is the most powerful road-going Pagani ever produced and it‚Äôs limited to just 99 examples worldwide. The cost for one is a cool $2.5 million.',\n",
       " \"The name Countach may be iconic, but is it worth $2.6 million? That's what Lamborghini hopes you will spend on its new Aventador-based model. Debuting in August of 2021, the Countach LPI 800-4 has a 6.5-liter V-12 with a light hybrid assist that gives it a whopping 803 horsepower, allowing for a 60 time of 2.8 seconds and a top speed of more than 220 miles per hour.\",\n",
       " \"The Koenigsegg Jesko, apart from being the fastest car the company has made to date, is also the most expensive. At $3.0 million out of the box, the handsome hypercar has 1,600-horsepower courtesy of the brand's ubiquitous twin-turbocharged 5.0-liter V-8 engine. Opt for the Absolut model, and the Jesko, in theory, will be able to hit 300 miles per hour.\",\n",
       " 'The Aspark Owl is all about its big numbers. The electric hypercar has 1,985 horsepower and 1,475 pound-feet of torque, which rockets it to 60 mph in a claimed 1.69 seconds. It can reach 186 mph in 10.6 seconds while on the way to its 249 mph top speed. All that power requires big, 10-piston front brakes and four-piston rear to keep it under control. They are limited, but you can buy one for a cool $3.1 million.',\n",
       " 'The Pagani Huayra Evo R is somehow a hotter version of the Huayra R track car that the automaker launched in 2021. It debuted earlier this year with an updated version of the Huayra R‚Äôs naturally aspirated, 6.0-liter V-12 engine, which makes 900 horsepower and 567 pound-feet of torque. Pagani also tweaked the Evo‚Äôs aerodynamics, which resulted in a 45 percent increase in downforce. There isn‚Äôt an official price on the Evo R, but estimates put it at around $3.5 million.',\n",
       " 'In several ways, the Sian represents a bridge to Lamborghini‚Äôs future. Though it‚Äôs based on the Aventador SVJ, this wild-looking Lambo is the brand‚Äôs first production electrified offering. In addition to the SVJ-sourced 6.5-liter V-12, the Sian uses a 48-volt mild-hybrid system. Total system output is 819 horsepower, which also makes it the most powerful Lamborghini ever. As confirmed by the 63 stuck on either side of the Sian‚Äôs wing, Lamborghini will produce just 63 units. And each will cost far more than the Aventador SVJ, carrying a price of $3.6 million.',\n",
       " 'The GMA T.50s Niki Lauda has hundreds of new parts compared to the standard T.50, making the car even more capable on the track. The unique car features a new aerodynamics package that increases downforce and a different version of the Cosworth-sourced 3.9-liter V-12 engine, making 761 horsepower at 11,500 rpm. It‚Äôs also the priciest of the Gordon Murray lineup at $3.9 million.',\n",
       " 'The Pagani Huayra Roadster arrived several years after the coupe, and the Roadster BC showed up a few years later in 2019. The V-12 in this version churns out 791 horsepower and 775 pound-feet‚Äîmore than the standard Roadster‚Äîand gets a slight weight reduction. It cost a cool $4.0 million when new.',\n",
       " 'The Bugatti Mistral sends the iconic W16 engine off in style. With 1,577 horsepower and no roof, the Mistral takes the best bits of the Chiron and borrows elements from the Bolide and Divo to create a truly unique roadster. Of course, Bugatti plans to build just 99 examples of the Mistral at the cost of around $5.0 million in the US‚Äîand all of them are already accounted for.',\n",
       " 'Among Bugatti‚Äôs recently debuted vehicles, the Divo is a staff favorite. Though it shares much in common with its cheaper (!) sibling the Chiron, the Divo has a lot going for it to justify the extra money. By adding lighter wheels, a carbon fiber intercooler and removing some sound deadening, Bugatti made the Divo 77 pounds lighter than the Chiron. Though power is unchanged from the Chiron‚Äôs 1,500 ponies, The Divo features a different aerodynamic setup, which makes it 8 seconds quicker around the Nardo test track. Finally, the moment you‚Äôve been waiting for: Bugatti is making 40 examples of the car, each costing $5.8 million.',\n",
       " 'Based on the Huayra, the Pagani Imola has an AMG-sourced 6.0-liter, twin-turbo V-12 engine that makes 838 horsepower and 811 pound-feet of torque. And it weighs just 2,776 pounds. The numerous fins, wings, and scoops help cool all the components and feed the thirsty engine, propelling it to an electronically limited 217-mph top speed. Pagani says the coupe costs $5.4 million and the Roadster is even more expensive at $6.0 million.',\n",
       " 'The long list of pricey (new) Paganis ends with the Codualunga. Inspired by 1960s Italian coachbuilding, the stunning exterior and steampunk-esque interior‚Äîwhile equally stunning‚Äîare only part of what makes this Pagani so pricey. Under the hood is a twin-turbocharged 6.0-liter V-12 producing 829 horsepower and 809 pound-feet of torque, paired to a seven-speed sequential transmission for a real race-car‚Äìlike feel. The cost for this one-of-five Pagani is $7.4 million.',\n",
       " \"Like many others before it, the Mercedes-Benz Maybach Exelero is a one-off. Commissioned by Fulda, a German subsidiary of Goodyear, to test its new tires, the Exlero debuted in 2004. Mercedes built the Exelero on the bones of a Maybach, and gave it the same twin-turbo V-12 engine producing 690 horsepower and 752 pound-feet of torque. Top speed is listed at 218 miles per hour and adjusted for inflation, the Exelero would cost more than $10 million in today's money.\",\n",
       " 'Bugatti debuted the Centodieci at last year‚Äôs Pebble Beach car week, showing off yet another ultra-rare, super-expensive model to the world. Limited to just 10‚Äîdieci‚Äîunits, the car is a modern throwback to the Bugatti EB110. At the same time, it‚Äôs meant to celebrate Bugatti‚Äôs momentous 110-year anniversary. Its unique styling cues won‚Äôt make everyone fall in love, but at least you won‚Äôt have to worry about seeing another one on the road. Carrying a price just shy of $9 million, the Centodiece is one of the most exclusive cars ever made.',\n",
       " \"Rolls-Royce, expectedly, takes two of the top three spots on this list, with the stunning Sweptail one-off from 2017 coming in at number three. With a monumental price tag of $13.0 million, it was, at the time of its debut, the most expensive new car ever. It's been outpriced since then, but the 453-horsepower luxury car is still a jaw-dropper.\",\n",
       " \"With a price tag of $13.4 million, the one-off Bugatti La Voiture Noire is officially the most expensive new Bugatti ever made. And understandably so. A modern interpretation of Jean Bugatti's personal Type 57 SC Atlantic, the La Voiture Noire uses the same quad-turbocharged 8.0-liter W-16 engine as the Chiron, producing 1,479 horsepower. It has six‚Äîthat's right, six exhaust tips‚Äîradical new wheels, an aggressive, bespoke fascia, and a gigantic light-up badge in the rear that spells out the name of the brand. Of course, this one-of-one Bugatti already has a home.\",\n",
       " 'The most expensive car in the world‚Äîofficially‚Äîis the Bugatti La Voiture Noire. With a price tag of $18.7 million after taxes, the one-off Bugatti La Voiture Noire is officially the priciest new car ever.',\n",
       " 'As of 2022, Tesla is still the world‚Äôs most valuable car brand with a valuation of $694 billion. Toyota, at second, has a valuation of $216 billion but outsells Tesla by a significant margin globally.']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = []\n",
    "\n",
    "# Use a relative XPath to select all price elements\n",
    "# Make sure to adjust the class name based on the actual HTML\n",
    "prc = driver.find_elements(By.XPATH, '//p[contains(text(),\"$\")]')  # Adjust class name accordingly\n",
    "\n",
    "# Loop through the selected price elements\n",
    "for j in prc:\n",
    "    pr = j.text\n",
    "    price.append(pr)\n",
    "\n",
    "# Print the number of prices scraped\n",
    "print(len(price))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "34b55847",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[408], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     car_prices\u001b[38;5;241m.\u001b[39mappend(price\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCar Name\u001b[39m\u001b[38;5;124m'\u001b[39m: car_names,\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m: car_prices\n\u001b[0;32m     44\u001b[0m })\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Step 1: Set up WebDriver and open the motor1 website\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.motor1.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Step 2: Search for '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, 'q')  # Find the search bar by its name attribute\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()  # Press enter to search\n",
    "\n",
    "# Step 3: Click on the relevant result for \"50 Most Expensive Cars in the World\"\n",
    "time.sleep(3)  # Wait for the search results to load\n",
    "result_link = driver.find_element(By.XPATH, '//a[contains(text(), \"50 Most Expensive Cars in the World\")]')\n",
    "result_link.click()\n",
    "\n",
    "# Step 4: Scrape the data (Car Name and Price)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Initialize lists to store the data\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# Assuming the cars and their prices are within certain elements on the page\n",
    "cars = driver.find_elements(By.XPATH, '//h2')  # Assuming car names are in h2 elements\n",
    "prices = driver.find_elements(By.XPATH, '//p[contains(text(),\"$\")]')  # Assuming prices contain \"$\" symbol in p tags\n",
    "\n",
    "# Step 5: Extract car names and prices\n",
    "for car in cars[:50]:  # Scrape only the first 50 cars\n",
    "    car_names.append(car.text)\n",
    "\n",
    "for price in prices[:50]:  # Scrape only the first 50 prices\n",
    "    car_prices.append(price.text)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Car Name': car_names,\n",
    "    'Price': car_prices\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Step 6: Save the data to an Excel file\n",
    "df.to_excel('50_most_expensive_cars.xlsx', index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "af48fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Step 1: Set up WebDriver and open the motor1 website\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.motor1.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Step 2: Search for '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, 'q')  # Find the search bar by its name attribute\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()  # Press enter to search\n",
    "\n",
    "# Step 3: Click on the relevant result for \"50 Most Expensive Cars in the World\"\n",
    "time.sleep(3)  # Wait for the search results to load\n",
    "result_link = driver.find_element(By.XPATH, '//a[contains(text(), \"50 Most Expensive Cars in the World\")]')\n",
    "result_link.click()\n",
    "\n",
    "# Step 4: Scrape the data (Car Name and Price)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Initialize lists to store the data\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# Assuming the cars and their prices are within certain elements on the page\n",
    "cars = driver.find_elements(By.XPATH, '//h2')  # Assuming car names are in h2 elements\n",
    "prices = driver.find_elements(By.XPATH, '//p[contains(text(),\"$\")]')  # Assuming prices contain \"$\" symbol in p tags\n",
    "\n",
    "# Step 5: Extract car names and prices (Ensure same length by padding empty values if missing)\n",
    "for car in cars[:50]:  # Scrape only the first 50 cars\n",
    "    car_names.append(car.text)\n",
    "\n",
    "for price in prices[:50]:  # Scrape only the first 50 prices\n",
    "    car_prices.append(price.text)\n",
    "\n",
    "# If lengths are unequal, pad the shorter list with empty strings\n",
    "while len(car_names) < len(car_prices):\n",
    "    car_names.append('')\n",
    "\n",
    "while len(car_prices) < len(car_names):\n",
    "    car_prices.append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "08548afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of car names: 1\n",
      "Number of car prices: 25\n",
      "                                            Car Name  \\\n",
      "0  From Aston Martin to Zenvo, these are the most...   \n",
      "\n",
      "                                               Price  \n",
      "0  But to find out which of these unattainable-to...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Step 1: Set up WebDriver and open the motor1 website\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.motor1.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Step 2: Search for '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, 'q')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()\n",
    "\n",
    "# Step 3: Click on the relevant result\n",
    "time.sleep(3)\n",
    "result_link = driver.find_element(By.XPATH, '//a[contains(text(), \"50 Most Expensive Cars in the World\")]')\n",
    "result_link.click()\n",
    "\n",
    "# Step 4: Scrape the data\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize lists to store the data\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# Scrape car names\n",
    "cars = driver.find_elements(By.XPATH, '//h2')\n",
    "for car in cars[:50]:  # Scrape only the first 50 cars\n",
    "    car_names.append(car.text)\n",
    "\n",
    "# Scrape prices\n",
    "prices = driver.find_elements(By.XPATH, '//p[contains(text(),\"$\")]')\n",
    "for price in prices[:50]:  # Scrape only the first 50 prices\n",
    "    car_prices.append(price.text)\n",
    "\n",
    "# Debugging: Print lengths of lists\n",
    "print(f\"Number of car names: {len(car_names)}\")\n",
    "print(f\"Number of car prices: {len(car_prices)}\")\n",
    "\n",
    "# Check if lengths match\n",
    "min_length = min(len(car_names), len(car_prices))\n",
    "car_names = car_names[:min_length]\n",
    "car_prices = car_prices[:min_length]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Car Name': car_names,\n",
    "    'Price': car_prices\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Step 6: Save the data to an Excel file\n",
    "df.to_excel('50_most_expensive_cars.xlsx', index=False, engine='openpyxl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
